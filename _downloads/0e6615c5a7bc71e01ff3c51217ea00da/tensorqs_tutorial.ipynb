{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MpIQ8xcaPYav"
      },
      "outputs": [],
      "source": [
        "# For tips on running notebooks in Google Colab, see\n",
        "# https://docs.pytorch.org/tutorials/beginner/colab\n",
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rNrcoJ0iPYax"
      },
      "source": [
        "[Learn the Basics](intro.html) \\|\\|\n",
        "[Quickstart](quickstart_tutorial.html) \\|\\| **Tensors** \\|\\| [Datasets &\n",
        "DataLoaders](data_tutorial.html) \\|\\|\n",
        "[Transforms](transforms_tutorial.html) \\|\\| [Build\n",
        "Model](buildmodel_tutorial.html) \\|\\|\n",
        "[Autograd](autogradqs_tutorial.html) \\|\\|\n",
        "[Optimization](optimization_tutorial.html) \\|\\| [Save & Load\n",
        "Model](saveloadrun_tutorial.html)\n",
        "\n",
        "Tensors\n",
        "=======\n",
        "\n",
        "Tensors are a specialized data structure that are very similar to arrays\n",
        "and matrices. In PyTorch, we use tensors to encode the inputs and\n",
        "outputs of a model, as well as the model's parameters.\n",
        "\n",
        "Tensors are similar to [NumPy's](https://numpy.org/) ndarrays, except\n",
        "that tensors can run on GPUs or other hardware accelerators. In fact,\n",
        "tensors and NumPy arrays can often share the same underlying memory,\n",
        "eliminating the need to copy data (see\n",
        "`bridge-to-np-label`{.interpreted-text role=\"ref\"}). Tensors are also\n",
        "optimized for automatic differentiation (we\\'ll see more about that\n",
        "later in the [Autograd](autogradqs_tutorial.html) section). If you're\n",
        "familiar with ndarrays, you'll be right at home with the Tensor API. If\n",
        "not, follow along!\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "v5atGFTEPYaz"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nxlFpBZxPYa0"
      },
      "source": [
        "# Initializing a Tensor : 텐서(tensor) 초기화\n",
        "\n",
        "**Directly from data : 데이터로부터 직접(directly) 생성하기**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "Ipu5jQAqPYa0"
      },
      "outputs": [],
      "source": [
        "data = [[1, 2],[3, 4]]\n",
        "x_data = torch.tensor(data)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2uF18e3_PYa0"
      },
      "source": [
        "**From a NumPy array : NumPy 배열로부터 생성하기**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "UYD69blCPYa1"
      },
      "outputs": [],
      "source": [
        "np_array = np.array(data)\n",
        "x_np = torch.from_numpy(np_array)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f5H8AYe8PYa1"
      },
      "source": [
        "**From another tensor: 다른 텐서로부터 생성하기**\n",
        "\n",
        "명시적으로 재정의(override)하지 않는다면, 인자로 주어진 텐서의 속성(모양(shape), 자료형(datatype))을 유지합니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UL_CgFg3ZBHn",
        "outputId": "18da7085-ed46-4ab7-c2be-1593f9f60e19"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Ones Tensor: \n",
            " tensor([[1, 1],\n",
            "        [1, 1]]) \n",
            "\n",
            "Random Tensor: \n",
            " tensor([[0.4423, 0.5923],\n",
            "        [0.2797, 0.8955]]) \n",
            "\n"
          ]
        }
      ],
      "source": [
        "x_ones = torch.ones_like(x_data) # x_data의 속성을 유지합니다.\n",
        "print(f\"Ones Tensor: \\n {x_ones} \\n\")\n",
        "\n",
        "x_rand = torch.rand_like(x_data, dtype=torch.float) # x_data의 속성을 덮어씁니다.\n",
        "print(f\"Random Tensor: \\n {x_rand} \\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mRmgfm5kPYa2"
      },
      "source": [
        "**With random or constant values: 무작위(random) 또는 상수(constant) 값을 사용하기**\n",
        "\n",
        "`shape` 은 텐서의 차원(dimension)을 나타내는 튜플(tuple)로, 아래 함수들에서는 출력 텐서의 차원을 결정합니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "m2mfboq8PYa3",
        "outputId": "d185142a-3df2-4008-daad-6778e21fa7f1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Random Tensor: \n",
            " tensor([[0.9490, 0.8327, 0.7027],\n",
            "        [0.7312, 0.0071, 0.1895]]) \n",
            "\n",
            "Ones Tensor: \n",
            " tensor([[1., 1., 1.],\n",
            "        [1., 1., 1.]]) \n",
            "\n",
            "Zeros Tensor: \n",
            " tensor([[0., 0., 0.],\n",
            "        [0., 0., 0.]])\n"
          ]
        }
      ],
      "source": [
        "shape = (2,3,)\n",
        "# 0 이상 1 미만의 무작위(random) 실수\n",
        "rand_tensor = torch.rand(shape)\n",
        "# 모든 원소가 1로 채워진 텐서\n",
        "ones_tensor = torch.ones(shape)\n",
        "# 모든 원소가 0으로 채워진 텐서\n",
        "zeros_tensor = torch.zeros(shape)\n",
        "\n",
        "print(f\"Random Tensor: \\n {rand_tensor} \\n\")\n",
        "print(f\"Ones Tensor: \\n {ones_tensor} \\n\")\n",
        "print(f\"Zeros Tensor: \\n {zeros_tensor}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "426V13gLPYa3"
      },
      "source": [
        "------------------------------------------------------------------------\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KmwubmpsPYa3"
      },
      "source": [
        "Attributes of a Tensor : 텐서의 속성(Attribute)\n",
        "======================\n",
        "\n",
        "Tensor attributes describe their shape, datatype, and the device on\n",
        "which they are stored.\n",
        "\n",
        "텐서의 속성은 텐서의 모양(shape), 자료형(datatype) 및 어느 장치에 저장되는지를 나타냅니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "UxCOm3n3PYa3",
        "outputId": "72cc1c5e-4eba-428b-f35b-e3bd0bfbf4a8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Shape of tensor: torch.Size([3, 4])\n",
            "Datatype of tensor: torch.float32\n",
            "Device tensor is stored on: cpu\n"
          ]
        }
      ],
      "source": [
        "tensor = torch.rand(3,4)\n",
        "\n",
        "print(f\"Shape of tensor: {tensor.shape}\")\n",
        "print(f\"Datatype of tensor: {tensor.dtype}\")\n",
        "print(f\"Device tensor is stored on: {tensor.device}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ol--ZtjLPYa3"
      },
      "source": [
        "------------------------------------------------------------------------\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oIyfSRLkPYa3"
      },
      "source": [
        "Operations on Tensors : 텐서 연산(Operation)\n",
        "=====================\n",
        "\n",
        "Over 1200 tensor operations, including arithmetic, linear algebra,\n",
        "matrix manipulation (transposing, indexing, slicing), sampling and more\n",
        "are comprehensively described\n",
        "[here](https://pytorch.org/docs/stable/torch.html).\n",
        "\n",
        "Each of these operations can be run on the CPU and\n",
        "[Accelerator](https://pytorch.org/docs/stable/torch.html#accelerators)\n",
        "such as CUDA, MPS, MTIA, or XPU. If you're using Colab, allocate an\n",
        "accelerator by going to Runtime \\> Change runtime type \\> GPU.\n",
        "\n",
        "By default, tensors are created on the CPU. We need to explicitly move\n",
        "tensors to the accelerator using `.to` method (after checking for\n",
        "accelerator availability). Keep in mind that copying large tensors\n",
        "across devices can be expensive in terms of time and memory!\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "cXqp9Rq1PYa3"
      },
      "outputs": [],
      "source": [
        "# We move our tensor to the current accelerator if available\n",
        "if torch.accelerator.is_available():\n",
        "    tensor = tensor.to(torch.accelerator.current_accelerator())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_BhztG1rPYa3"
      },
      "source": [
        "Try out some of the operations from the list. If you\\'re familiar with\n",
        "the NumPy API, you\\'ll find the Tensor API a breeze to use.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PaUXCsMmPYa3"
      },
      "source": [
        "**Standard numpy-like indexing and slicing:**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "4ZATRNm_PYa4",
        "outputId": "48d52d72-5400-4807-8562-0e96155d850f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "First row: tensor([1., 1., 1., 1.])\n",
            "First column: tensor([1., 1., 1., 1.])\n",
            "Last column: tensor([1., 1., 1., 1.])\n",
            "tensor([[1., 0., 1., 1.],\n",
            "        [1., 0., 1., 1.],\n",
            "        [1., 0., 1., 1.],\n",
            "        [1., 0., 1., 1.]])\n"
          ]
        }
      ],
      "source": [
        "# 텐서 생성 - 모든 원소 1로 초기화\n",
        "tensor = torch.ones(4, 4)\n",
        "# 인덱싱(Indexing)\n",
        "print(f\"First row: {tensor[0]}\") # 첫 번째 행(row) 전체를 선택\n",
        "print(f\"First column: {tensor[:, 0]}\") # 모든 행(:)의 첫 번째 열(column) 만 선택\n",
        "print(f\"Last column: {tensor[..., -1]}\") # 모든 차원(...)에서 마지막(-1) 열(column) 을 선택\n",
        "#                           ~~~~~ ellipsis\n",
        "\n",
        "# 슬라이싱(Slicing) 및 수정\n",
        "tensor[:,1] = 0 # 모든 행(:)의 두 번째 열(1번째 열)을 0으로 바꿈\n",
        "print(tensor)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RbUi7nbZPYa4"
      },
      "source": [
        "**Joining tensors** You can use `torch.cat` to concatenate a sequence of\n",
        "tensors along a given dimension. See also\n",
        "[torch.stack](https://pytorch.org/docs/stable/generated/torch.stack.html),\n",
        "another tensor joining operator that is subtly different from\n",
        "`torch.cat`.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3WpIHNniPYa4",
        "outputId": "3d3c0995-1d5f-4db2-f066-9a130f7326a7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[1., 0., 1., 1., 1., 0., 1., 1., 1., 0., 1., 1.],\n",
            "        [1., 0., 1., 1., 1., 0., 1., 1., 1., 0., 1., 1.],\n",
            "        [1., 0., 1., 1., 1., 0., 1., 1., 1., 0., 1., 1.],\n",
            "        [1., 0., 1., 1., 1., 0., 1., 1., 1., 0., 1., 1.]])\n"
          ]
        }
      ],
      "source": [
        "t1 = torch.cat([tensor, tensor, tensor], dim=1)\n",
        "print(t1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1d004f5d-4af4-4fb0-ec16-989246d10211",
        "id": "IeJw5mWPef13"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([4, 4])\n"
          ]
        }
      ],
      "source": [
        "tensor = torch.tensor([\n",
        "    [1., 0., 1., 1.],\n",
        "    [1., 0., 1., 1.],\n",
        "    [1., 0., 1., 1.],\n",
        "    [1., 0., 1., 1.]\n",
        "])\n",
        "print(tensor.shape)   # (4, 4)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "t_stack0 = torch.stack([tensor, tensor, tensor], dim=0)\n",
        "print(t_stack0.shape)"
      ],
      "metadata": {
        "id": "KlYnsKvyemzB",
        "outputId": "4386d1a2-5de0-47be-ed2e-852e736bbde6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([3, 4, 4])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "torch.cat()과 torch.stack()을 이해해볼 수 있는 예제 코드"
      ],
      "metadata": {
        "id": "ZwOiYy9_pVW6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "# 1️⃣ 예시 텐서 3개 생성 (모두 shape=(2, 2))\n",
        "A = torch.tensor([[1, 2],\n",
        "                  [3, 4]])\n",
        "B = torch.tensor([[5, 6],\n",
        "                  [7, 8]])\n",
        "C = torch.tensor([[9, 10],\n",
        "                  [11, 12]])\n",
        "\n",
        "print(\"A:\\n\", A)\n",
        "print(\"B:\\n\", B)\n",
        "print(\"C:\\n\", C)\n",
        "print(\"-\" * 40)\n",
        "print(\"모두 shape:\", A.shape)"
      ],
      "metadata": {
        "id": "mOI99d7kpQ95",
        "outputId": "71274e42-c23e-478d-bab4-3ae8af924499",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "A:\n",
            " tensor([[1, 2],\n",
            "        [3, 4]])\n",
            "B:\n",
            " tensor([[5, 6],\n",
            "        [7, 8]])\n",
            "C:\n",
            " tensor([[ 9, 10],\n",
            "        [11, 12]])\n",
            "----------------------------------------\n",
            "모두 shape: torch.Size([2, 2])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# dim=0 (행 방향, 아래로)\n",
        "cat_dim0 = torch.cat([A, B, C], dim=0)\n",
        "print(\"torch.cat(dim=0) → 행 방향 결합:\")\n",
        "print(cat_dim0)\n",
        "print(\"shape:\", cat_dim0.shape, \"\\n\")  # (6, 2)\n",
        "\n",
        "# dim=1 (열 방향, 옆으로)\n",
        "cat_dim1 = torch.cat([A, B, C], dim=1)\n",
        "print(\"torch.cat(dim=1) → 열 방향 결합:\")\n",
        "print(cat_dim1)\n",
        "print(\"shape:\", cat_dim1.shape, \"\\n\")  # (2, 6)"
      ],
      "metadata": {
        "id": "UE0uGRqNpUds",
        "outputId": "9c71fdfd-9e03-459f-fdd4-9b14e1ee5e93",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.cat(dim=0) → 행 방향 결합:\n",
            "tensor([[ 1,  2],\n",
            "        [ 3,  4],\n",
            "        [ 5,  6],\n",
            "        [ 7,  8],\n",
            "        [ 9, 10],\n",
            "        [11, 12]])\n",
            "shape: torch.Size([6, 2]) \n",
            "\n",
            "torch.cat(dim=1) → 열 방향 결합:\n",
            "tensor([[ 1,  2,  5,  6,  9, 10],\n",
            "        [ 3,  4,  7,  8, 11, 12]])\n",
            "shape: torch.Size([2, 6]) \n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# dim=0 : 새 축을 맨 앞에 추가 (위로 쌓기)\n",
        "stack_dim0 = torch.stack([A, B, C], dim=0)\n",
        "print(\"torch.stack(dim=0) → 새로운 축(맨 앞)에 쌓기:\")\n",
        "print(stack_dim0)\n",
        "print(\"shape:\", stack_dim0.shape, \"\\n\")  # (3, 2, 2)\n",
        "\n",
        "# dim=1 : 중간에 새 축 추가\n",
        "stack_dim1 = torch.stack([A, B, C], dim=1)\n",
        "print(\"torch.stack(dim=1) → 두 번째 차원에 쌓기:\")\n",
        "print(stack_dim1)\n",
        "print(\"shape:\", stack_dim1.shape, \"\\n\")  # (2, 3, 2)\n",
        "\n",
        "# dim=2 : 마지막에 새 축 추가\n",
        "stack_dim2 = torch.stack([A, B, C], dim=2)\n",
        "print(\"torch.stack(dim=2) → 마지막 축에 쌓기:\")\n",
        "print(stack_dim2)\n",
        "print(\"shape:\", stack_dim2.shape)  # (2, 2, 3)"
      ],
      "metadata": {
        "id": "_LXK_N1Kq7Zg",
        "outputId": "b7487f26-7bda-410b-a2bb-70ed7ed0511e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.stack(dim=0) → 새로운 축(맨 앞)에 쌓기:\n",
            "tensor([[[ 1,  2],\n",
            "         [ 3,  4]],\n",
            "\n",
            "        [[ 5,  6],\n",
            "         [ 7,  8]],\n",
            "\n",
            "        [[ 9, 10],\n",
            "         [11, 12]]])\n",
            "shape: torch.Size([3, 2, 2]) \n",
            "\n",
            "torch.stack(dim=1) → 두 번째 차원에 쌓기:\n",
            "tensor([[[ 1,  2],\n",
            "         [ 5,  6],\n",
            "         [ 9, 10]],\n",
            "\n",
            "        [[ 3,  4],\n",
            "         [ 7,  8],\n",
            "         [11, 12]]])\n",
            "shape: torch.Size([2, 3, 2]) \n",
            "\n",
            "torch.stack(dim=2) → 마지막 축에 쌓기:\n",
            "tensor([[[ 1,  5,  9],\n",
            "         [ 2,  6, 10]],\n",
            "\n",
            "        [[ 3,  7, 11],\n",
            "         [ 4,  8, 12]]])\n",
            "shape: torch.Size([2, 2, 3])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B4Q30vgMPYa4"
      },
      "source": [
        "**Arithmetic operations**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FyAE8S2XPYa4",
        "outputId": "8ddf3dcb-0877-4280-bcdd-1ae8f0f1a3bd"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[1., 0., 1., 1.],\n",
              "        [1., 0., 1., 1.],\n",
              "        [1., 0., 1., 1.],\n",
              "        [1., 0., 1., 1.]])"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ],
      "source": [
        "# This computes the matrix multiplication between two tensors. y1, y2, y3 will have the same value\n",
        "# ``tensor.T`` returns the transpose of a tensor\n",
        "y1 = tensor @ tensor.T\n",
        "y2 = tensor.matmul(tensor.T)\n",
        "\n",
        "y3 = torch.rand_like(y1)\n",
        "torch.matmul(tensor, tensor.T, out=y3)\n",
        "\n",
        "\n",
        "# This computes the element-wise product. z1, z2, z3 will have the same value\n",
        "z1 = tensor * tensor\n",
        "z2 = tensor.mul(tensor)\n",
        "\n",
        "z3 = torch.rand_like(tensor)\n",
        "torch.mul(tensor, tensor, out=z3)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qz3B_g_hPYa4"
      },
      "source": [
        "**Single-element tensors** If you have a one-element tensor, for example\n",
        "by aggregating all values of a tensor into one value, you can convert it\n",
        "to a Python numerical value using `item()`:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LO0OgtAzPYa4"
      },
      "outputs": [],
      "source": [
        "agg = tensor.sum()\n",
        "agg_item = agg.item()\n",
        "print(agg_item, type(agg_item))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H5tsAQlvPYa4"
      },
      "source": [
        "**In-place operations** Operations that store the result into the\n",
        "operand are called in-place. They are denoted by a `_` suffix. For\n",
        "example: `x.copy_(y)`, `x.t_()`, will change `x`.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qZQB5uYgPYa4"
      },
      "outputs": [],
      "source": [
        "print(f\"{tensor} \\n\")\n",
        "tensor.add_(5)\n",
        "print(tensor)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KIeHOH6qPYa4"
      },
      "source": [
        "<div style=\"background-color: #54c7ec; color: #fff; font-weight: 700; padding-left: 10px; padding-top: 5px; padding-bottom: 5px\"><strong>NOTE:</strong></div>\n",
        "\n",
        "<div style=\"background-color: #f3f4f7; padding-left: 10px; padding-top: 10px; padding-bottom: 10px; padding-right: 10px\">\n",
        "\n",
        "<p>In-place operations save some memory, but can be problematic when computing derivatives because of an immediate lossof history. Hence, their use is discouraged.</p>\n",
        "\n",
        "</div>\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "85QHVbTTPYa5"
      },
      "source": [
        "------------------------------------------------------------------------\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qlq3vh8fPYa5"
      },
      "source": [
        "Bridge with NumPy {#bridge-to-np-label}\n",
        "=================\n",
        "\n",
        "Tensors on the CPU and NumPy arrays can share their underlying memory\n",
        "locations, and changing one will change the other.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lflrvGpFPYa5"
      },
      "source": [
        "Tensor to NumPy array\n",
        "=====================\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eH98iYKYPYa5"
      },
      "outputs": [],
      "source": [
        "t = torch.ones(5)\n",
        "print(f\"t: {t}\")\n",
        "n = t.numpy()\n",
        "print(f\"n: {n}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ERDhdAePPYa5"
      },
      "source": [
        "A change in the tensor reflects in the NumPy array.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wLWJFYN2PYa5"
      },
      "outputs": [],
      "source": [
        "t.add_(1)\n",
        "print(f\"t: {t}\")\n",
        "print(f\"n: {n}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uNaC9USbPYa5"
      },
      "source": [
        "NumPy array to Tensor\n",
        "=====================\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UswzuUA-PYa5"
      },
      "outputs": [],
      "source": [
        "n = np.ones(5)\n",
        "t = torch.from_numpy(n)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dUn1KZ3CPYa5"
      },
      "source": [
        "Changes in the NumPy array reflects in the tensor.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mz-IlFB0PYa5"
      },
      "outputs": [],
      "source": [
        "np.add(n, 1, out=n)\n",
        "print(f\"t: {t}\")\n",
        "print(f\"n: {n}\")"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}